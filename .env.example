# Model Configuration
MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct

# Quantization: 4bit (8GB VRAM), 8bit (16GB VRAM), none (40GB+ VRAM)
QUANTIZATION=4bit

# Vector Database
VECTOR_DB_PATH=data/vectorstore

# Generation Parameters
MAX_TOKENS=512
TEMPERATURE=0.7

# Hugging Face Token (required for Llama models)
# Get your token from: https://huggingface.co/settings/tokens
# HF_TOKEN=your_huggingface_token_here

# Optional: Model variants you can try on RunPod
# Llama 3.1 70B (requires 40GB+ VRAM with 4bit)
# MODEL_NAME=meta-llama/Llama-3.1-70B-Instruct
# QUANTIZATION=4bit
# MAX_TOKENS=1024

# Llama 3.3 70B (requires 40GB+ VRAM with 4bit)
# MODEL_NAME=meta-llama/Llama-3.3-70B-Instruct
# QUANTIZATION=4bit
# MAX_TOKENS=1024